import requests
from lxml import html
from concurrent.futures import ThreadPoolExecutor, as_completed
from logger import get_logger
from config import CATALOG_URL, TOTAL_THREADS
import time


class Parser:
    def __init__(self, proxy_manager, user_agent_manager):
        self.proxy_manager = proxy_manager
        self.user_agent_manager = user_agent_manager
        self.logger = get_logger()
        self.session = self._create_session()


    def _create_session(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é requests —Å –æ–±—â–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –ø—Ä–æ–∫—Å–∏.
        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–æ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö.
        """
        session = requests.Session()
        user_agent = self.user_agent_manager.get_random_user_agent()
        proxy = self.proxy_manager.get_random_proxy()

        session.headers.update({"User-Agent": user_agent})
        if proxy:
            session.proxies.update({"http": proxy, "https": proxy})

        self.logger.info(f"üîß –°–æ–∑–¥–∞–Ω–∞ —Å–µ—Å—Å–∏—è —Å User-Agent: {user_agent} –∏ –ø—Ä–æ–∫—Å–∏: {proxy}")
        return session


    def _get_response(self, url, timeout=15):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç GET-–∑–∞–ø—Ä–æ—Å —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        self.logger.info(f"üåê –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {url}")
        try:
            start_time = time.time()
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            elapsed_time = time.time() - start_time
            self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç (–°—Ç–∞—Ç—É—Å: {response.status_code}, –í—Ä–µ–º—è: {elapsed_time:.2f}s)")
            return response
        except requests.Timeout:
            self.logger.error(f"‚è≥ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {url}")
        except requests.RequestException as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
        return None

    def get_last_page(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞."""
        self.logger.info(f"üîç –ó–∞–ø—Ä–æ—Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü: {CATALOG_URL}")
        response = self._get_response(CATALOG_URL, timeout=10)

        if not response:
            self.logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            return 1

        tree = html.fromstring(response.content)
        page_numbers = [int(num) for num in tree.xpath('//a[contains(@href, "?PAGEN_1=")]/text()') if num.isdigit()]
        last_page = max(page_numbers) if page_numbers else 1
        self.logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {last_page}")
        return last_page

    def parse_catalog_page(self, page_num):
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä—ã —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞."""
        url = f"{CATALOG_URL}?PAGEN_1={page_num}"
        response = self._get_response(url)

        if not response:
            self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∑–∞–ø—Ä–æ—Å–∞")
            return []

        content = response.content.decode("utf-8")
        tree = html.fromstring(content)
        products = tree.xpath('//a[contains(@class, "catalog-item") and contains(@class, "js-catalog-item")]')
        self.logger.info(f"üîé –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")

        items = [self.extract_product_info(product, page_num) for product in products]
        self.logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞. –°–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(items)}")
        return items

    def extract_product_info(self, product, page_num):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ –∏–∑ HTML-—ç–ª–µ–º–µ–Ω—Ç–∞."""
        try:
            name = product.xpath('.//div[contains(@class, "carusel-wrap-line-center-item-list-item-name")]/span/text()')
            price = product.xpath('.//span[contains(@class, "carusel-wrap-line-center-item-list-item-price")]/text()')
            old_price = product.xpath(
                './/span[contains(@class, "carusel-wrap-line-center-item-list-item-price-old")]/i/text()')
            img_url = product.xpath('.//img[contains(@class, "first_img_s")]/@src')
            product_url = product.xpath('./@href')

            product_info = {
                "–ù–∞–∑–≤–∞–Ω–∏–µ": name[0].strip() if name else "-",
                "–¶–µ–Ω–∞": price[0].strip() if price else "-",
                "–°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞": old_price[0].strip() if old_price else "–ù–µ—Ç —Å—Ç–∞—Ä–æ–π —Ü–µ–Ω—ã",
                "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": img_url[0] if img_url else "–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
                "–°—Å—ã–ª–∫–∞": product_url[0] if product_url else "–ù–µ—Ç —Å—Å—ã–ª–∫–∏"
            }

            self.logger.debug(f"üì¶ (–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}) –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ç–æ–≤–∞—Ä: {product_info}")
            return product_info
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–æ–≤–∞—Ä–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}: {e}")
            return {
                "–ù–∞–∑–≤–∞–Ω–∏–µ": "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞",
                "–¶–µ–Ω–∞": "-",
                "–°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞": "-",
                "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": "-",
                "–°—Å—ã–ª–∫–∞": "-"
            }

    def parse_all_pages(self):
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é."""
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
        last_page = self.get_last_page()
        self.logger.info(f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {last_page}")
        all_items = []

        with ThreadPoolExecutor(max_workers=TOTAL_THREADS) as executor:
            futures = {executor.submit(self.parse_catalog_page, page): page for page in range(1, last_page + 1)}

            for future in as_completed(futures):
                page = futures[future]
                try:
                    start_time = time.time()
                    items = future.result()
                    elapsed_time = time.time() - start_time
                    all_items.extend(items)
                    self.logger.info(
                        f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Ä–µ–º—è: {elapsed_time:.2f}s. –°–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(items)}")
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")

        self.logger.info(f"üèÅ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_items)}")
        return all_items