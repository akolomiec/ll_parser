import requests
from lxml import html
import pandas as pd
import json
import random
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Semaphore

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
USER_AGENT_FILE = "useragent.txt"
PROXY_FILE = "proxies.txt"
CSV_FILE = "catalog.csv"
LOG_FILE = "parser.log"

# URL –∫–∞—Ç–∞–ª–æ–≥–∞
CATALOG_URL = "https://liniilubvi.ru/catalog/"


# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
TOTAL_THREADS = 2

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def load_user_agents():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ User-Agent –∏–∑ JSON."""
    try:
        with open(USER_AGENT_FILE, "r", encoding="utf-8") as f:
            agents = json.load(f)
            return [ua["ua"] for ua in agents]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ User-Agent
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ User-Agent: {e}")
        return ["Mozilla/5.0"]  # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å–ª–æ–º–∞–Ω

def get_random_user_agent():
    """–í—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –∏–∑ —Å–ø–∏—Å–∫–∞."""
    return random.choice(user_agents)


def load_proxies():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏"""
    try:
        with open(PROXY_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–∫—Å–∏: {e}")
        return []

def get_random_user_agent():
    """–í—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π User-Agent"""
    return random.choice(user_agents)

def get_random_proxy():
    """–í—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)"""
    return random.choice(proxies) if proxies else None

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
user_agents = load_user_agents()
proxies = load_proxies()


def get_last_page():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    headers = {"User-Agent": get_random_user_agent()}
    proxy = get_random_proxy()
    proxy_dict = {"http": proxy, "https": proxy} if proxy else {}

    logging.info(f"üåê –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ {CATALOG_URL}")

    try:
        response = requests.get(CATALOG_URL, headers=headers, proxies=proxy_dict, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return 1

    tree = html.fromstring(response.content)

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    page_numbers = tree.xpath('//a[contains(@href, "?PAGEN_1=")]/text()')

    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞
    page_numbers = [int(num) for num in page_numbers if num.isdigit()]

    last_page = max(page_numbers) if page_numbers else 1
    logging.info(f"üìå –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {last_page}")
    return last_page


def parse_catalog_page(page_num, proxy):
    """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞"""
    url = f"{CATALOG_URL}?PAGEN_1={page_num}"
    headers = {"User-Agent": get_random_user_agent()}
    proxy_dict = {"http": proxy, "https": proxy} if proxy else {}

    logging.info(f"üåç –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num} —á–µ—Ä–µ–∑ {proxy}")

    try:
        response = requests.get(url, headers=headers, proxies=proxy_dict, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
        return []
    content = response.content.decode('utf-8')
    tree = html.fromstring(content)
    items = []

    # –ù–æ–≤—ã–π XPath –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤
    products = tree.xpath('//a[contains(@class, "catalog-item") and contains(@class, "js-catalog-item")]')

    for product in products:
        try:
            name = product.xpath('.//div[contains(@class, "carusel-wrap-line-center-item-list-item-name")]/span/text()')
            price = product.xpath('.//span[contains(@class, "carusel-wrap-line-center-item-list-item-price")]/text()')
            old_price = product.xpath('.//span[contains(@class, "carusel-wrap-line-center-item-list-item-price-old")]/i/text()')
            img_url = product.xpath('.//img[contains(@class, "first_img_s")]/@src')
            product_url = product.xpath('.//@href')

            if name and price:
                item = {
                    "–ù–∞–∑–≤–∞–Ω–∏–µ": name[0].strip(),
                    "–¶–µ–Ω–∞": price[0].strip(),
                    "–°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞": old_price[0].strip() if old_price else "–ù–µ—Ç —Å—Ç–∞—Ä–æ–π —Ü–µ–Ω—ã",
                    "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": img_url[0] if img_url else "–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
                    "–°—Å—ã–ª–∫–∞": product_url[0] if product_url else "–ù–µ—Ç —Å—Å—ã–ª–∫–∏"
                }
                items.append(item)
                logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ç–æ–≤–∞—Ä: {item}")

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞: {e}")

    return items


def parse_all_pages():
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–Ω–æ–≥–æ–ø–æ—Ç–æ–∫–µ"""
    last_page = get_last_page()
    logging.info(f"üìå –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {last_page} —Å—Ç—Ä–∞–Ω–∏—Ü —Å {TOTAL_THREADS} –ø–æ—Ç–æ–∫–∞–º–∏")

    all_items = []
    with ThreadPoolExecutor(max_workers=TOTAL_THREADS) as executor:
        future_to_page = {executor.submit(parse_catalog_page, page, get_random_proxy()): page for page in range(1, last_page + 1)}

        for future in as_completed(future_to_page):
            page = future_to_page[future]
            try:
                items = future.result()
                all_items.extend(items)
                logging.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞, —Ç–æ–≤–∞—Ä–æ–≤: {len(items)}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")

    logging.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ–±—Ä–∞–Ω–æ: {len(all_items)}")
    return all_items

def save_to_csv(data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ CSV"""
    df = pd.DataFrame(data)
    df.to_csv(CSV_FILE, sep=";", encoding="cp1251", index=False)
    logging.info(f"üìÇ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {CSV_FILE}")

if __name__ == "__main__":
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞")
    data = parse_all_pages()
    save_to_csv(data)
    logging.info("üèÅ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
