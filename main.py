import requests
from lxml import html
import pandas as pd
import time
import json
import random
import logging

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
USER_AGENT_FILE = "useragent.txt"
CSV_FILE = "catalog.csv"
LOG_FILE = "parser.log"

# URL –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞
CATALOG_URL = "https://liniilubvi.ru/catalog/"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,  # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º INFO, WARNING –∏ ERROR
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),  # –õ–æ–≥ –≤ —Ñ–∞–π–ª
        logging.StreamHandler()  # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    ]
)

def load_user_agents(filename=USER_AGENT_FILE):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ User-Agent –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            ua_list = json.load(f)
            logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ua_list)} User-Agent'–æ–≤")
            return [ua["ua"] for ua in ua_list]  # –î–æ—Å—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å User-Agent
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ user-agent'–æ–≤: {e}")
        return ["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.3"]  # –ó–∞–ø–∞—Å–Ω–æ–π UA

def get_random_user_agent(ua_list):
    """–í—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –∏–∑ —Å–ø–∏—Å–∫–∞"""
    return random.choice(ua_list)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ User-Agent'–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
user_agents = load_user_agents()

def get_last_page():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –∫–∞—Ç–∞–ª–æ–≥–µ"""
    headers = {"User-Agent": get_random_user_agent(user_agents)}

    logging.info(f"üåê –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º {CATALOG_URL} –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü")
    try:
        response = requests.get(CATALOG_URL, headers=headers)
        response.raise_for_status()
    except requests.RequestException as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {CATALOG_URL}: {e}")
        return 1

    tree = html.fromstring(response.content)
    page_numbers = tree.xpath('//*[@id="content"]/div[2]/div[4]/div/section/div[3]/div[2]/div/a/text()')
    page_numbers = [int(num) for num in page_numbers if num.isdigit()]

    last_page = max(page_numbers) if page_numbers else 1
    logging.info(f"üìå –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {last_page}")
    return last_page

def parse_catalog_page(page_num):
    url = f"{CATALOG_URL}?PAGEN_1={page_num}"
    ua = get_random_user_agent(user_agents)
    print(f"üì¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è User-Agent: {ua}")
    headers = {"User-Agent": ua}

    response = requests.get(url, headers=headers)
    content = response.content.decode('utf-8')


    if response.status_code != 200:
        print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")
        return []

    tree = html.fromstring(content)
    items = []

    for i in range(1, 21):
        try:
            # –ì–∏–±–∫–∏–π XPath: –∏—â–µ–º –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ç–æ–≤–∞—Ä–∞
            name_xpath = f'//*[@id="catalog-list"]/a[{i}]//div[contains(@class, "carusel-wrap-line-center-item-list-item-name")]//span/text()'
            name = tree.xpath(name_xpath)

            # XPath –¥–ª—è —Ü–µ–Ω—ã
            price_xpath = f'//*[@id="catalog-list"]/a[{i}]//span[contains(@class, "carusel-wrap-line-center-item-list-item-price")]/text()'
            price = tree.xpath(price_xpath)

            if name and price:
                item = {"–ù–∞–∑–≤–∞–Ω–∏–µ": name[0].strip(), "–¶–µ–Ω–∞": price[0].strip()}
                items.append(item)
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ç–æ–≤–∞—Ä: {item}")
            else:
                print(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω —ç–ª–µ–º–µ–Ω—Ç {i}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞ {i}: {e}")

    return items


def parse_all_pages():
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
    last_page = get_last_page()
    logging.info(f"üìå –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {last_page} —Å—Ç—Ä–∞–Ω–∏—Ü")

    all_items = []
    for page in range(1, last_page + 1):
        logging.info(f"üì• –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} –∏–∑ {last_page}...")
        items = parse_catalog_page(page)
        all_items.extend(items)
        time.sleep(random.uniform(1, 3))  # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏

    logging.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ–±—Ä–∞–Ω–æ: {len(all_items)}")
    return all_items

def save_to_csv(data, filename=CSV_FILE):
    """ –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π """
    df = pd.DataFrame(data)
    df.to_csv(filename, sep = ';', encoding='cp1251')
    logging.info(f"üìÇ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

if __name__ == "__main__":
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞")
    data = parse_all_pages()
    save_to_csv(data)
    logging.info("üèÅ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
